# Amendment 02: RubyLLM + MCP as Internal Dependencies

## Problem

Current AI extension treats `ruby_llm` and MCP as optional/abstract:

1. Abstract `Provider` class with pluggable implementations
2. `Provider.current=` configuration dance
3. MCP executor is a placeholder stub
4. "Is gem loaded?" checks everywhere
5. No real MCP server connectivity

This adds complexity without benefit. These gems ARE the implementation.

## Solution

Make `ruby_llm` and `mcp` (official Anthropic MCP SDK) **required runtime dependencies**:

- Direct RubyLLM API calls in Agent executor
- Real MCP::Client for MCP executor
- Remove provider abstraction layer
- Simpler, more powerful, actually works

---

## Gem Details

### ruby_llm

- **Gem:** `ruby_llm`
- **Repo:** https://github.com/crmne/ruby_llm
- **Features:** Multi-provider (OpenAI, Anthropic, Gemini, etc.), chat, streaming, tools, embeddings, moderation

```ruby
# Chat
chat = RubyLLM.chat
response = chat.ask("Hello")

# Streaming
chat.ask("Tell me a story") { |chunk| print chunk.content }

# Tools
class MyTool < RubyLLM::Tool
  description "Does something"
  param :input, desc: "The input"
  def execute(input:)
    # return result
  end
end
chat.with_tool(MyTool).ask("Use the tool")

# Moderation
RubyLLM.moderate("content to check")
```

### mcp (Anthropic Ruby SDK)

- **Gem:** `mcp`
- **Repo:** https://github.com/modelcontextprotocol/ruby-sdk
- **Maintainers:** Anthropic + Shopify

```ruby
# Client usage
transport = MCP::Client::HTTP.new(url: "https://server.example.com/mcp")
client = MCP::Client.new(transport: transport)

# List tools
tools = client.tools
tools.each { |t| puts "#{t.name}: #{t.description}" }

# Call tool
result = client.call_tool(tool: tools.first, arguments: { foo: "bar" })

# With auth
transport = MCP::Client::HTTP.new(
  url: "https://server.example.com/mcp",
  headers: { "Authorization" => "Bearer token" }
)
```

---

## TODO

### Phase 1 - Gemspec & Dependencies

- [ ] **Add runtime dependencies to gemspec**

  ```ruby
  spec.add_dependency "ruby_llm", "~> 1.0"
  spec.add_dependency "mcp", "~> 0.1"
  spec.add_dependency "faraday", ">= 2.0"  # Required by MCP HTTP client
  ```

- [ ] **Update Gemfile**
  - Move ruby_llm from development to runtime
  - Add mcp gem

### Phase 2 - Remove Provider Abstraction

- [ ] **Delete `lib/durable_workflow/extensions/ai/provider.rb`**

- [ ] **Delete `lib/durable_workflow/extensions/ai/providers/` directory**

- [ ] **Update `lib/durable_workflow/extensions/ai/ai.rb`**

  - Remove `Provider.current` / `Provider.current=`
  - Remove `AI.setup(provider:)` - no longer needed
  - Add `AI.configure` for RubyLLM config (API keys, default model)

  ```ruby
  module AI
    class << self
      attr_accessor :default_model

      def configure(api_key: nil, model: nil)
        RubyLLM.configure { |c| c.openai_api_key = api_key } if api_key
        @default_model = model || "gpt-4o-mini"
      end

      def chat(model: nil)
        RubyLLM.chat(model: model || default_model)
      end
    end
  end
  ```

### Phase 3 - Rewrite Agent Executor

- [ ] **Update `lib/durable_workflow/extensions/ai/executors/agent.rb`**

  - Direct RubyLLM usage, no provider indirection
  - Convert AgentDef tools to RubyLLM::Tool subclasses dynamically
  - Support streaming via block

  ```ruby
  def call(state)
    agent = resolve_agent(config.agent_id)
    chat = AI.chat(model: agent.model)

    # Add tools
    agent.tools.each { |t| chat.with_tool(build_tool_class(t)) }

    # Build messages
    messages = build_messages(state, agent)

    # Execute (with optional streaming)
    response = if config.stream && @stream_handler
      chat.ask(messages) { |chunk| @stream_handler.call(chunk) }
    else
      chat.ask(messages)
    end

    store_and_continue(state, response)
  end

  private

  def build_tool_class(tool_def)
    # Dynamically create RubyLLM::Tool subclass from ToolDef
    Class.new(RubyLLM::Tool) do
      description tool_def.description
      tool_def.parameters.each { |p| param p.name, desc: p.description }

      define_method(:execute) do |**args|
        # Call the service method defined in ToolDef
        svc = Object.const_get(tool_def.service)
        svc.public_send(tool_def.method_name, **args)
      end
    end
  end
  ```

### Phase 4 - Rewrite MCP Executor

- [ ] **Update `lib/durable_workflow/extensions/ai/executors/mcp.rb`**

  - Real MCP::Client implementation
  - Connection pooling for servers
  - Tool discovery and invocation

  ```ruby
  class MCP < Base
    Registry.register("mcp", self)

    # Server connection cache
    @clients = {}

    def self.client_for(server_config)
      @clients[server_config[:url]] ||= begin
        transport = ::MCP::Client::HTTP.new(
          url: server_config[:url],
          headers: server_config[:headers] || {}
        )
        ::MCP::Client.new(transport: transport)
      end
    end

    def call(state)
      server_config = resolve_server(config.server)
      client = self.class.client_for(server_config)

      # Find tool
      tool = client.tools.find { |t| t.name == config.tool }
      raise ExecutionError, "MCP tool not found: #{config.tool}" unless tool

      # Resolve arguments
      args = resolve(state, config.arguments || {})

      # Call tool
      result = client.call_tool(tool: tool, arguments: args)

      state = store(state, config.output, result)
      continue(state, output: result)
    end

    private

    def resolve_server(server_id)
      servers = AI.data_from(workflow)[:mcp_servers] || {}
      servers[server_id.to_sym] || raise(ExecutionError, "MCP server not found: #{server_id}")
    end
  end
  ```

### Phase 5 - Rewrite Guardrail Executor

- [ ] **Update `lib/durable_workflow/extensions/ai/executors/guardrail.rb`**

  - Use `RubyLLM.moderate` directly for moderation check

  ```ruby
  def check_moderation(content)
    result = RubyLLM.moderate(content)
    GuardrailResult.new(
      passed: !result.flagged?,
      check_type: "moderation",
      reason: result.flagged? ? "Content flagged: #{result.categories.join(', ')}" : nil
    )
  end
  ```

### Phase 6 - Update AI Types

- [ ] **Update `lib/durable_workflow/extensions/ai/types.rb`**

  - Add MCPServerConfig type

  ```ruby
  class MCPServerConfig < BaseStruct
    attribute :url, Types::Strict::String
    attribute? :headers, Types::Hash.default({}.freeze)
    attribute? :name, Types::Strict::String.optional
  end
  ```

- [ ] **Update MCPConfig**
  ```ruby
  class MCPConfig < StepConfig
    attribute :server, Types::Strict::String  # Server ID from workflow config
    attribute :tool, Types::Strict::String    # Tool name to call
    attribute? :arguments, Types::Hash.default({}.freeze)
    attribute? :output, Types::Coercible::Symbol.optional
  end
  ```

### Phase 7 - Parser Updates

- [ ] **Update AI extension parser hooks**
  - Parse `mcp_servers` section from workflow YAML
  ```yaml
  mcp_servers:
    github:
      url: "https://mcp.github.com/v1"
      headers:
        Authorization: "Bearer ${GITHUB_TOKEN}"
    slack:
      url: "https://mcp.slack.com/v1"
  ```

### Phase 8 - Update Tests

- [ ] **Delete `test/unit/extensions/ai/provider_test.rb`**

- [ ] **Delete `test/unit/extensions/ai/providers/` directory**

- [ ] **Update `test/unit/extensions/ai/executors/agent_test.rb`**

  - Mock at RubyLLM level, not provider level
  - Test RubyLLM::Tool dynamic class generation

- [ ] **Update `test/unit/extensions/ai/executors/mcp_test.rb`**

  - Mock MCP::Client
  - Test real tool discovery and invocation flow

- [ ] **Update `test/unit/extensions/ai/executors/guardrail_test.rb`**

  - Mock RubyLLM.moderate for moderation tests

- [ ] **Add `test/support/mcp_mock.rb`**

  - Mock MCP::Client for tests

  ```ruby
  class MockMCPClient
    def initialize(tools: [])
      @tools = tools
    end

    def tools
      @tools
    end

    def call_tool(tool:, arguments:)
      # Return mock result
    end
  end
  ```

### Phase 9 - Documentation

- [ ] **Update README usage examples**
  - Show RubyLLM configuration
  - Show MCP server configuration in workflow YAML
  - Remove provider setup docs

---

## File Changes Summary

| Action | File                                                        |
| ------ | ----------------------------------------------------------- |
| DELETE | `lib/durable_workflow/extensions/ai/provider.rb`            |
| DELETE | `lib/durable_workflow/extensions/ai/providers/ruby_llm.rb`  |
| DELETE | `lib/durable_workflow/extensions/ai/providers/` directory   |
| DELETE | `test/unit/extensions/ai/provider_test.rb`                  |
| MODIFY | `durable_workflow.gemspec` (add deps)                       |
| MODIFY | `Gemfile` (add deps)                                        |
| MODIFY | `lib/durable_workflow/extensions/ai/ai.rb`                  |
| MODIFY | `lib/durable_workflow/extensions/ai/types.rb`               |
| MODIFY | `lib/durable_workflow/extensions/ai/executors/agent.rb`     |
| MODIFY | `lib/durable_workflow/extensions/ai/executors/mcp.rb`       |
| MODIFY | `lib/durable_workflow/extensions/ai/executors/guardrail.rb` |
| MODIFY | `test/unit/extensions/ai/executors/*.rb`                    |
| CREATE | `test/support/mcp_mock.rb`                                  |

---

## Workflow YAML Example (After)

```yaml
id: customer_support
name: AI Customer Support
version: "1.0"

# MCP servers available to this workflow
mcp_servers:
  zendesk:
    url: "https://mcp.zendesk.com/v1"
    headers:
      Authorization: "Bearer ${ZENDESK_TOKEN}"

agents:
  support_agent:
    model: gpt-4o
    instructions: "You are a helpful support agent..."
    tools: [lookup_order, create_ticket]

tools:
  lookup_order:
    description: "Look up order by ID"
    parameters:
      - name: order_id
        type: string
        required: true
    service: OrderService
    method: find

  create_ticket:
    description: "Create support ticket"
    parameters:
      - name: subject
        type: string
      - name: body
        type: string
    service: TicketService
    method: create

steps:
  - id: start
    type: start
    next: check_input

  - id: check_input
    type: guardrail
    content: "$input.message"
    checks:
      - type: prompt_injection
      - type: moderation
    on_fail: rejected
    next: get_context

  - id: get_context
    type: mcp
    server: zendesk
    tool: get_customer_context
    arguments:
      customer_id: "$input.customer_id"
    output: customer_context
    next: respond

  - id: respond
    type: agent
    agent_id: support_agent
    prompt: "$input.message"
    output: response
    next: end

  - id: rejected
    type: assign
    set:
      response: "I cannot process this request."
    next: end

  - id: end
    type: end
    result: "$response"
```

---

## Acceptance Criteria

1. ✅ `ruby_llm` is runtime dependency, not optional
2. ✅ `mcp` gem is runtime dependency
3. ✅ No `Provider` abstraction - direct RubyLLM calls
4. ✅ MCP executor actually connects to servers
5. ✅ MCP executor discovers and calls tools
6. ✅ Agent executor uses RubyLLM::Tool for tool definitions
7. ✅ Guardrail uses RubyLLM.moderate directly
8. ✅ Workflow YAML can define `mcp_servers` section
9. ✅ All tests pass with mocked RubyLLM/MCP at library level
10. ✅ Streaming works via RubyLLM blocks
